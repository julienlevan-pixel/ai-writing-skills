# CIHR Grant Writing Skill

## Overview

A section-by-section guide for writing CIHR Project Grant applications, modeled on the structure of the PANTHEON SLIM (Randomized Controlled Trial) grant and cross-referenced with the CardioAgent (AI/Technology) grant. This skill is generalizable across clinical trials, observational studies, AI/technology development, and other health research study designs.

Each section includes: **Requirements** (what must be present), **Expectations** (what reviewers look for), **Scoring Rubric** (weighted scoring criteria), and **Templates** (fill-in-the-blank scaffolds).



## SECTION 1: The Need for a Trial / Study

**Weight: 25/100**

This section must build the case that your study addresses a critical, unmet need. It follows a hierarchical argument structure.

---

### 1.1 What is the problem to be addressed?

**Weight: 5/25**

#### Requirements

- [ ] State the clinical/scientific problem in 2-3 sentences maximum
- [ ] Define the specific patient population or target group
- [ ] Identify what is currently unknown or what practice gap exists
- [ ] State why current evidence is insufficient (e.g., "no RCT has addressed...", "current AI tools lack...")
- [ ] Use underline/bold formatting to emphasize the key population and the key gap

#### Expectations

- The problem statement should be immediately understandable to a non-specialist reviewer
- Frame as a patient-centered or health-system problem, not just a scientific curiosity
- Quantify the problem where possible (mortality rates, error rates, prevalence)
- Explicitly state the knowledge gap that this study will fill

#### Template

> The problem to be addressed is that [specific clinical/scientific gap] in [target population] is unknown/unresolved. [Current approach or standard of care] represents [limitation], but [it has only been studied in X / no evidence supports Y / current tools cannot Z]. [Quantify the consequence of not addressing this gap].

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Clarity of problem | 2/5 | Immediately clear, specific, and compelling | Understandable but somewhat vague | Confusing or overly broad |
| Evidence of gap | 2/5 | Systematic review or guideline gap cited; no existing RCT/study identified | Literature cited but gap not well-defined | No evidence that gap exists |
| Patient/population focus | 1/5 | Specific population clearly defined with prevalence data | Population mentioned but not well-characterized | No clear target population |

---

### 1.2 What are the principal research questions to be addressed?

**Weight: 5/25**

#### Requirements

- [ ] State the **overarching objective** in one sentence (use the study acronym if applicable)
- [ ] State the **central hypothesis** clearly
- [ ] Define the **primary objective** with a feasibility/success threshold (for pilot) or primary endpoint (for phase III)
- [ ] List **secondary objectives** (numbered, 2-4 items)
- [ ] List **exploratory objectives** (numbered, 2-5 items)
- [ ] For each objective level (primary/secondary/exploratory), state whether results will be evaluated by sex and gender subgroups
- [ ] For pilot/feasibility studies: primary objective MUST be operational (e.g., recruitment rate), not clinical

#### Expectations

- Objectives must follow a clear hierarchy: Primary > Secondary > Exploratory
- The primary objective must be answerable with the proposed design and sample size
- For pilot studies: clinical outcomes are exploratory only (state this explicitly)
- Sex and gender analysis commitment should be present at every objective level
- Use CIHR-specific language: "patient-oriented," "sex and gender-based analysis (SGBA+)"

#### Template

> **Overarching objective:** The overarching objective of the "[STUDY ACRONYM]" [study type] is to [determine/evaluate/develop] [intervention/technology] [in what population] [to achieve what].
>
> **Central hypothesis:** We hypothesize that [intervention/approach] will [expected effect] compared with [comparator] in [population].
>
> **Primary objective:** To [specific measurable objective]. **Success threshold:** [define].
>
> **Secondary objectives:**
> i. To [objective 1];
> ii. To [objective 2].
>
> **Exploratory objectives:**
> i. To [objective 1];
> ii. To [objective 2];
> iii. To [objective 3].

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Hypothesis clarity | 2/5 | Testable, specific, directional hypothesis with clear comparator | Hypothesis present but vague | No hypothesis or untestable statement |
| Objective hierarchy | 2/5 | Clear primary with feasible threshold; logical secondary and exploratory | Objectives present but hierarchy unclear | Objectives conflated or missing levels |
| SGBA+ integration | 1/5 | Sex/gender analysis specified at each objective level | Mentioned but not integrated into objectives | Absent |

---

### 1.3 Why is a trial/study needed now?

**Weight: 10/25**

This is the most critical subsection of the entire "Need" section. It builds the scientific rationale through a logical chain of evidence. Use numbered subsections (1.3.1, 1.3.2, etc.).

#### Requirements

- [ ] **1.3.1 Burden of disease/problem:** Canadian and global epidemiological data with citations
- [ ] **1.3.2 Current standard of care/approach:** What is currently done and why it works (partially)
- [ ] **1.3.3 Limitations of current approach:** Why the standard of care is insufficient (quantify risks, error rates, costs)
- [ ] **1.3.4 Emerging evidence for the proposed approach:** What new evidence suggests a better strategy exists
- [ ] **1.3.5 Evidence gap in the target population:** Demonstrate that the proposed approach has NOT been studied in your specific population (cite systematic review or guideline gap)
- [ ] **1.3.6 Specific technical/clinical gap:** Address any unique safety or feasibility concerns
- [ ] **1.3.7 Sex and gender representation gap:** Demonstrate that prior studies lacked adequate sex/gender representation and analysis
- [ ] Use bold/underline for the key concluding statement of the evidence gap
- [ ] Include at least one citation from your own team's prior work in this field

#### Expectations

- Build the argument like a legal brief: each subsection leads logically to the next
- Use Canadian data prominently (CIHR is a Canadian funder) or primarily
- Demonstrate clinical equipoise with real-world data (e.g., practice variation showing disagreement)
- The final statement should be a bold, underlined, definitive assertion of the knowledge gap
- Reference your own prior work to show you are the right team to address this gap
- For AI/technology studies: address current limitations of existing AI approaches and why yours is different

#### Template for Key Concluding Statement

> **There is thus an important knowledge gap to support the use of [proposed approach] in [target population], despite the fact that this population represents [quantify the size/importance of the population].**

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Burden of disease (Canadian context) | 2/10 | Canadian-specific epidemiology with recent data and economic impact | Some Canadian data but incomplete | No Canadian data or only global |
| Logical argument chain | 3/10 | Each subsection flows naturally to the next; unavoidable conclusion | Generally logical but some jumps | Disjointed; conclusion not supported |
| Evidence of gap | 3/10 | Systematic review or guideline development process cited showing no existing evidence | Literature review shows gap but not systematic | Assertion without evidence |
| Own prior work cited | 1/10 | Multiple relevant team publications integrated into rationale | One team publication mentioned | No team publications cited |
| Sex/gender gap identified | 1/10 | Specific data on under-representation with quantification | General statement about sex/gender gaps | Not mentioned |

---

### 1.4 How will the results of this trial/study be used?

**Weight: 3/25**

#### Requirements

- [ ] For pilot studies: state that results will inform the design of a phase III confirmatory study
- [ ] For confirmatory studies: state that results will influence clinical guidelines
- [ ] Describe the dissemination plan: conferences, journals, social media, public engagement
- [ ] State whether the study could transition to the next phase seamlessly (adaptive design)
- [ ] Explain why CIHR funding is required (e.g., no industry interest due to generic drugs / public health focus)
- [ ] Name the specific guideline body or clinical practice that would be impacted

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Impact pathway | 2/3 | Clear pipeline from pilot to phase III to guidelines with named bodies | General pathway described | Vague impact statement |
| Dissemination plan | 1/3 | Multiple channels: conferences, journals, media, patient engagement | Basic plan (journal + conference) | No plan |

---

### 1.5 Are there any risks to the safety of participants?

**Weight: 2/25**

#### Requirements

- [ ] State the risk profile of the intervention explicitly
- [ ] For de-escalation studies: emphasize that no NEW risks are expected
- [ ] For technology studies: address privacy, data security, and potential for misdiagnosis
- [ ] Acknowledge theoretical risks and cite evidence that these have NOT been observed in comparable populations
- [ ] Describe monitoring mechanisms (DSMB, adverse event reporting)

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Risk characterization | 1/2 | Comprehensive, evidence-based risk assessment with mitigation | Risks mentioned with some mitigation | Risks not addressed or dismissed |
| Safety monitoring | 1/2 | DSMB, adverse event protocol, stopping rules defined | Some monitoring described | No safety monitoring plan |

---

## SECTION 2: The Proposed Trial / Study Design

**Weight: 40/100**

This section covers all methodological details. Each subsection maps to a specific CIHR review criterion.

---

### 2.1 What is the proposed trial/study design?

**Weight: 5/40**

#### Requirements

- [ ] State the study design using standard terminology (e.g., "pilot, multi-center, double-blinded, pragmatic, patient-centered RCT")
- [ ] State whether it is a pilot/feasibility study or a confirmatory study
- [ ] If pragmatic: reference PRECIS-2 tool and justify pragmatic elements
- [ ] Describe patient engagement strategy: patient partners on steering committee, co-development of protocol
- [ ] For AI/technology: describe the validation framework (retrospective + prospective phases)
- [ ] Include a study flowchart figure

#### Template

> The proposed [study] is a [phase], [number of sites]-center, [blinding], [pragmatic/explanatory], and [patient-centered] [study type] designed to [primary purpose]. [Population] will be eligible. [Brief description of randomization/allocation]. The [pragmatic/explanatory] design ensures that [justification].

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Design appropriateness | 2/5 | Design perfectly matches research question; standard terminology used | Appropriate design but not fully justified | Design-question mismatch |
| Patient engagement | 2/5 | Named patient partner on steering committee; co-developed protocol; EDI principles | Patient input mentioned but not structured | No patient engagement |
| Study flowchart | 1/5 | Clear, comprehensive flowchart with all study phases and timelines | Flowchart present but incomplete | No flowchart |

---

### 2.2 What are the planned trial interventions?

**Weight: 4/40**

#### Requirements

- [ ] Describe the experimental intervention with dose/frequency/route/duration
- [ ] Describe the control intervention with the same level of detail
- [ ] Justify the choice of comparator with evidence and guideline references
- [ ] Address regulatory requirements (e.g., Health Canada Clinical Trial Application)
- [ ] Describe management of participants on prior therapies (switching protocols)
- [ ] State that all other treatments follow standard of care (pragmatic principle)
- [ ] Describe post-study care plan

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Intervention clarity | 2/4 | Fully specified with dose, route, frequency, duration for both arms | Mostly specified but some gaps | Vague intervention description |
| Comparator justification | 1/4 | Evidence-based with guideline reference; addresses evidence gaps in comparator choice | Some justification | No justification for comparator |
| Regulatory and practical | 1/4 | Regulatory pathway identified; switching protocols; post-study care | Some practical issues addressed | Regulatory/practical issues ignored |

---

### 2.3 Allocation to trial groups

**Weight: 2/40**

#### Requirements

- [ ] State allocation ratio (e.g., 1:1)
- [ ] State stratification variables with justification (cite validation of stratification tool)
- [ ] State block sizes
- [ ] Name the randomization platform/system
- [ ] For non-RCT designs: describe sampling or allocation strategy

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Randomization rigor | 2/2 | Validated stratification tool cited; appropriate block sizes; named platform | Randomization described but not fully detailed | No randomization details |

---

### 2.4 Methods for protecting against sources of bias

**Weight: 2/40**

#### Requirements

- [ ] Describe blinding strategy (who is blinded: participants, investigators, outcome assessors)
- [ ] Describe allocation concealment method
- [ ] For AI studies: describe blinding of human evaluators to AI outputs
- [ ] Address potential sources of bias specific to your design

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Bias protection | 2/2 | Double-blind with allocation concealment; all bias sources addressed | Single-blind or partial concealment | Open-label without justification |

---

### 2.5 Inclusion/exclusion criteria

**Weight: 3/40**

#### Requirements

- [ ] List inclusion criteria as bullet points
- [ ] List exclusion criteria as bullet points
- [ ] For pragmatic trials: explicitly state that few exclusion criteria are used to maximize generalizability
- [ ] Justify any exclusion criterion that removes a specific subpopulation
- [ ] Describe screening log data collection: baseline characteristics, sex, gender, race, ethnicity, reasons for exclusion
- [ ] Address EDI: describe strategies to include underrepresented populations (e.g., Indigenous communities)
- [ ] For heterogeneous populations: acknowledge heterogeneity and state how subgroups will be characterized

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Criteria appropriateness | 1/3 | Pragmatic criteria maximizing generalizability; each exclusion justified | Reasonable criteria but some unjustified exclusions | Overly restrictive or unjustified |
| EDI integration | 1/3 | Named strategies for diverse recruitment; screening log captures demographics | EDI mentioned but no concrete strategy | No EDI consideration |
| Population characterization | 1/3 | Heterogeneity acknowledged; subgroup plan described | Some acknowledgment | Assumed homogeneity |

---

### 2.6-2.7 Treatment duration and follow-up

**Weight: 2/40**

#### Requirements

- [ ] State treatment duration with start and end points
- [ ] State follow-up visit schedule with specific time points
- [ ] State visit modalities (in-person, telephone, video)
- [ ] State total study duration (recruitment + follow-up)
- [ ] State recruitment period duration

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Timeline completeness | 2/2 | All durations specified; flexible visit modalities; realistic timeline | Mostly specified | Incomplete or unrealistic |

---

### 2.8 Primary and secondary outcome measures

**Weight: 6/40**

#### Requirements

- [ ] **Primary outcome:** State with exact definition, measurement method, and success threshold
- [ ] **Secondary outcomes:** List each with definition (numbered, 2-5 items)
- [ ] **Exploratory outcomes:** List each with definition (numbered, 3-6 items)
- [ ] For pilot studies: primary outcome MUST be feasibility-related (recruitment rate, adherence, etc.)
- [ ] For clinical trials: use standardized endpoint definitions (e.g., Academic Research Consortium, BARC bleeding)
- [ ] For AI studies: define accuracy metrics (AUROC, F1, sensitivity, specificity) and reference standards
- [ ] Include patient-oriented outcomes
- [ ] For novel endpoints: describe the development methodology (e.g., discrete-choice experiment)

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Primary outcome definition | 2/6 | Precisely defined with validated measurement method and threshold | Defined but threshold unclear | Vague or inappropriate primary outcome |
| Outcome hierarchy | 2/6 | Clear primary/secondary/exploratory with appropriate scope at each level | Hierarchy present but some misclassification | No hierarchy or outcomes conflated |
| Standardized definitions | 1/6 | All endpoints use published consensus definitions with citations | Most endpoints standardized | Custom definitions without justification |
| Patient-oriented outcomes | 1/6 | Named patient-oriented outcomes with development methodology | Patient outcomes mentioned | No patient-oriented outcomes |

---

### 2.9 How will outcomes be measured at follow-up?

**Weight: 2/40**

#### Requirements

- [ ] State the data source for each outcome category (screening logs, medical charts, self-report, imaging)
- [ ] State whether endpoints will be adjudicated (and by whom) or not adjudicated (justify for pilot)
- [ ] Describe adverse event and serious adverse event monitoring
- [ ] For AI studies: describe the reference standard / ground truth generation process (e.g., central reader model)

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Measurement rigor | 2/2 | Each outcome has specified data source; adjudication plan clear; AE monitoring | Mostly specified | Measurement methods unclear |

---

### 2.10 Sample size justification

**Weight: 4/40**

#### Requirements

- [ ] For pilot/feasibility studies: state explicitly that formal sample size calculation is not applicable; provide expected enrollment range based on recruitment assumptions
- [ ] For confirmatory studies: provide full power calculation with alpha, beta, effect size, and assumptions
- [ ] State the basis for effect size assumptions (prior studies, pilot data, clinical significance)
- [ ] Address multiple comparisons if applicable
- [ ] For AI studies: justify the number of cases for training/validation/testing; describe stratified sampling for rare conditions

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Sample size appropriateness | 2/4 | Fully justified with transparent assumptions; sensitivity analyses | Calculation present but assumptions not fully justified | No calculation or unrealistic assumptions |
| Effect size basis | 2/4 | Based on own pilot data or meta-analysis of comparable studies | Based on literature but indirect evidence | Arbitrary or unjustified effect size |

---

### 2.11-2.14 Practical considerations

**Weight: 4/40**

Covers: health service research issues, recruitment, compliance, and loss to follow-up.

#### Requirements

- [ ] **Recruitment:** State expected rate per site per month with evidence; describe recruitment process; state total expected enrollment
- [ ] **Compliance:** Describe adherence monitoring strategy; cite expected adherence/discontinuation rates from prior trials; describe patient-partner involvement in adherence strategies
- [ ] **Loss to follow-up:** State expected rate with evidence; describe retention strategies; commit to identifying barriers
- [ ] For all three: state that sex/gender disparities will be monitored and mitigated

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Recruitment feasibility | 2/4 | Evidence-based rate; named sites with capacity; prior recruitment experience | Rate stated but evidence weak | No evidence of feasibility |
| Adherence/retention plan | 2/4 | Proactive monitoring with iterative strategies; patient partner involvement; sex/gender analysis | Basic plan | No plan |

---

### 2.15 Number of centers

**Weight: 1/40**

#### Requirements

- [ ] List all centers with site PI name and role
- [ ] Include geographic diversity (for Canadian multi-center: multiple provinces)
- [ ] Reference letters of support from each site
- [ ] For international sites: describe regulatory coordination plan

---

### 2.16-2.18 Analysis plan

**Weight: 5/40**

#### Requirements

- [ ] **Type of analyses:** For pilot: explicitly state "most analyses will be descriptive"; for confirmatory: state primary statistical test
- [ ] Describe analysis for each outcome level (primary, secondary, exploratory)
- [ ] State whether intention-to-treat or per-protocol analysis (or both)
- [ ] **Frequency of analyses:** State whether interim analyses are planned; describe DSMB access to data
- [ ] **Subgroup analyses:** List pre-specified subgroups (sex, gender, site, risk score); state whether interaction testing will be performed
- [ ] For AI studies: describe performance metrics, calibration, and fairness analyses across demographic subgroups

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Statistical rigor | 3/5 | Appropriate methods for each outcome; multiple comparison handling; ITT specified | Generally appropriate but some gaps | Inappropriate methods or no plan |
| Subgroup/SGBA+ | 2/5 | Pre-specified sex/gender subgroups at all objective levels; fairness metrics for AI | Some subgroup analysis planned | No subgroup analysis |

---

### 2.19 Prior pilot work / Preliminary data

**Weight: 2/40**

#### Requirements

- [ ] Describe all preliminary studies that inform this proposal (surveys, observational studies, pilot data)
- [ ] For each: state IRB status, funding source, and how results will inform the current study
- [ ] Demonstrate that these studies were conducted independently (no overlapping funds)
- [ ] For AI studies: present preliminary performance data in table format (model comparison)
- [ ] State how the totality of the preliminary program will inform the proposed study

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Preliminary evidence | 2/2 | Multiple complementary preliminary studies with IRB approval; results directly inform design | Some pilot work described | No preliminary work |

---

## SECTION 3: Trial / Study Management

**Weight: 15/100**

---

### 3.1 Day-to-day management arrangements

**Weight: 5/15**

#### Requirements

- [ ] Name the coordinating center / academic research organization (with experience record)
- [ ] Describe: contract management, site coordination, electronic case report forms, database management, data governance, medical monitoring, regulatory submissions, statistical analysis, DSMB coordination
- [ ] Describe monitoring plan (in-person vs. remote; frequency)
- [ ] Describe data security: source document storage, de-identification, platform name
- [ ] For AI studies: describe data pipeline, model deployment infrastructure, and privacy architecture (on-premises, VPC)

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Operational infrastructure | 3/5 | Named CRO/coordinating center with track record; all operational elements addressed | Most elements described | Vague or no operational plan |
| Data management and security | 2/5 | Named platform; de-identification protocol; monitoring plan | Basic data management described | No data management plan |

---

### 3.2 Role of each principal applicant and co-applicant

**Weight: 5/15**

#### Requirements

- [ ] For each team member: Name, degree, role (NPA/PA/Co-A), career stage, institution, specific contribution
- [ ] Nominated Principal Applicant: must be "ultimately responsible for all aspects"
- [ ] Describe the executive committee composition
- [ ] Show complementary expertise across team (clinical, methodological, statistical, patient engagement, regulatory, EDI)
- [ ] For multi-center: identify site PIs
- [ ] Reference letters of support

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Team completeness | 3/5 | All necessary expertise covered; each member has defined role; no redundancy | Most expertise covered | Key expertise gaps |
| Leadership clarity | 2/5 | NPA clearly responsible; executive committee defined; mentorship relationships explicit | Leadership structure unclear | No governance structure |

---

### 3.3 Steering committee and DSMB

**Weight: 5/15**

#### Requirements
THIS MIGHT BE OPTIONAL ACCROING TO TEH SUTDY

- [ ] **Steering committee:** List composition (executive committee + site PIs + patient partner)
- [ ] **DSMB:** State coordinating body; meeting frequency; charter development; composition (clinician chair + statistician minimum)
- [ ] For pilot studies: state that DSMB will NOT terminate for efficacy/futility but MAY terminate for safety
- [ ] For confirmatory studies: define stopping rules (efficacy, futility, safety)

#### Scoring Rubric

| Criterion | Weight | 5 (Excellent) | 3 (Good) | 1 (Weak) |
|-----------|--------|---------------|----------|----------|
| Governance structure | 3/5 | Complete steering committee with patient partner; DSMB with charter and stopping rules | Basic governance described | No governance structure |
| Safety monitoring | 2/5 | DSMB with appropriate scope for study phase; regular meetings | Some safety monitoring | No safety monitoring plan |

---

## SECTION 4: General Considerations (Cross-Cutting Themes)

**Weight: 10/100**

These themes must be woven throughout the proposal but are often evaluated as a standalone criterion.

---

### 4.1 Sex, Gender, and Equity (SGBA+)

**Weight: 4/10**

#### Requirements

- [ ] Define how sex and gender will be collected (female/male/other for sex; woman/man/X gender/other for gender)
- [ ] Commit to sex- and gender-stratified analyses at each objective level
- [ ] Collect gender-specific and sex-specific non-traditional risk factors
- [ ] Address historical under-representation of women/gender minorities in your field
- [ ] Describe mitigation strategies if disparities are observed
- [ ] For AI studies: describe fairness monitoring across demographic subgroups (demographic parity, equalized odds)
- [ ] State sampling strategies to achieve sex balance (e.g., "approximately 50% women")

---

### 4.2 Patient Engagement

**Weight: 3/10**

#### Requirements

- [ ] Name the patient partner(s) on the steering committee
- [ ] Describe their specific contributions (protocol co-development, recruitment strategies, adherence strategies, knowledge mobilization)
- [ ] Reference EDI framework used (e.g., CEPPP Learning Together Evaluation Framework)
- [ ] Describe patient-oriented outcome development (if applicable)
- [ ] Include letter of collaboration from patient engagement center

---

### 4.3 Knowledge Translation and Dissemination

**Weight: 3/10**

#### Requirements

- [ ] Describe publication plan: target journal tier, conference presentations
- [ ] Describe public engagement: social media, mainstream media, patient communities
- [ ] For AI studies: describe open-source release plans and platform accessibility
- [ ] State how results will be incorporated into clinical guidelines (name the guideline body)
- [ ] Describe regulatory pathway if applicable (Health Canada SaMD, Clinical Trial Application)

---

## OVERALL SCORING SUMMARY

| Section | Weight | Key Question |
|---------|--------|--------------|
| 0. Summary of Progress | 10/100 | Is this investigator the right person to do this work? |
| 1. The Need | 25/100 | Is this an important, unanswered question? |
| 2. The Proposed Study | 40/100 | Is the methodology rigorous and feasible? |
| 3. Trial Management | 15/100 | Can this team execute this study? |
| 4. General Considerations | 10/100 | Does this study address equity, patient engagement, and impact? |
| **TOTAL** | **100/100** | |

### Score Interpretation

- **90-100:** Fundable as-is. Minor revisions only.
- **75-89:** Competitive. Address reviewer concerns in specific sections.
- **60-74:** Needs significant revision. Major gaps in 1-2 sections.
- **Below 60:** Fundamental redesign needed.

---

## APPENDIX A: CIHR-Specific Language and Formatting Tips

1. **Use CIHR vocabulary:** "nominated principal applicant" (not "PI"), "principal applicant" (not "co-PI"), "co-applicant" (not "co-investigator"), "collaborator" (not "consultant")
2. **Sex and gender:** Always separate sex (biological) from gender (social construct); use CIHR SGBA+ framework
3. **Patient engagement:** Use "patient partner" not "patient representative"; reference CIHR Strategy for Patient-Oriented Research (SPOR)
4. **Knowledge translation:** This is CIHR's preferred term over "dissemination" or "implementation"
5. **Indigenous health:** If relevant, reference CIHR Institute of Indigenous Peoples' Health guidelines; use respectful terminology (First Nations, Inuit, Metis Peoples)
6. **Formatting:** Use numbered subsections (1.1, 1.2, etc.); bold/underline key statements; use bullet points for criteria; include figures/tables where possible
7. **Page limits:** Respect CIHR page limits strictly. The research proposal is typically limited to 13 pages for a Project Grant.
8. **References:** Use numbered Vancouver style; cite Canadian data prominently; cite your own team's work

---

## APPENDIX B: Adapting This Template by Study Type

### For Randomized Controlled Trials (PANTHEON model)
- Section 1.3 emphasizes clinical equipoise and treatment gap
- Section 2 uses full trial design subsections (2.1-2.19)
- DSMB is mandatory
- Pilot vs. phase III distinction is critical for objective hierarchy

### For AI/Technology Development Studies (CardioAgent model)
- Section 1.3 emphasizes diagnostic error rates and technology limitations
- Section 2 organizes around Aims (Aim 1: Technical Validation, Aim 2: Clinical Validation, Aim 3: Prospective Evaluation)
- Include a "Pilot Data" section with performance tables (F1, AUROC, sensitivity)
- Address privacy, on-premises deployment, and regulatory pathway (SaMD)
- Include fairness/bias monitoring framework

### For Observational Studies
- Section 1.3 emphasizes knowledge gap from existing observational data limitations
- Section 2 focuses on cohort definition, exposure/outcome definitions, confounding control
- No DSMB required; describe data governance instead
- Address causal inference limitations explicitly

### For Systematic Reviews and Meta-Analyses
- Section 1.3 emphasizes inconsistency in existing evidence
- Section 2 describes search strategy, inclusion/exclusion for studies, risk of bias assessment, synthesis methods
- Register protocol with PROSPERO
- Follow PRISMA guidelines

---

## APPENDIX C: Common Reviewer Critiques and How to Preempt Them

| Common Critique | How to Preempt |
|----------------|----------------|
| "The sample size is inadequate" | For pilot: explicitly state sample size calculation is not applicable and explain why. For confirmatory: provide sensitivity analyses. |
| "The team lacks expertise in X" | Ensure all key expertise areas are covered in Section 3.2; add co-applicants if gaps exist |
| "Sex and gender analysis is superficial" | Integrate SGBA+ at every objective level; collect sex AND gender separately; address non-traditional risk factors |
| "Patient engagement is tokenistic" | Name the patient partner; describe their specific contributions; reference CEPPP or SPOR framework |
| "The comparator is not justified" | Cite guidelines and evidence for comparator choice; acknowledge evidence gaps in comparator arm |
| "Clinical outcomes are not adjudicated" | For pilot: state explicitly that adjudication is not needed and justify. For confirmatory: describe full adjudication plan with central reader model |
| "The study is not feasible" | Provide evidence for recruitment rate; cite similar studies' recruitment; list site capacities |
| "Unclear how results will be used" | State the explicit pipeline: pilot -> phase III -> guidelines; name the guideline body |
| "Relying on AI to evaluate AI is risky" | Describe human adjudication with central reader model; specify sample size for adjudication; use subspecialty core labs |
| "The proposal lacks innovation" | Distinguish from prior work clearly; state what is novel about the approach, population, or endpoint |
